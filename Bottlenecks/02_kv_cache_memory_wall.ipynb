{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottleneck 2 — The KV Cache Memory Wall\n",
    "\n",
    "## The Problem\n",
    "\n",
    "During **autoregressive generation**, a Transformer predicts one token at a time.  \n",
    "For every new token, attention is computed against **all previous tokens**.  \n",
    "Without caching that would mean recomputing every Key/Value vector from scratch on each step — O(n²) work.\n",
    "\n",
    "The **KV Cache** trades memory for compute: we store K and V tensors for every past token, so each step only needs to compute K/V for the *new* token.\n",
    "\n",
    "### The trade-off that becomes a wall\n",
    "\n",
    "| Model | Layers | Heads | Head-dim | KV per token | @ 4k tokens | @ 32k tokens |\n",
    "|-------|--------|-------|----------|--------------|--------------|--------------|\n",
    "| 7B  | 32 | 32 | 128 | 512 KB | ~2 GB | ~16 GB |\n",
    "| 13B | 40 | 40 | 128 | 800 KB | ~3 GB | ~25 GB |\n",
    "| 70B | 80 | 64 | 128 | 2 MB   | ~8 GB | ~64 GB |\n",
    "\n",
    "A long conversation on a 70B model can **exceed the VRAM of 8 × A100 GPUs combined**.\n",
    "\n",
    "## Solutions We Will Explore\n",
    "\n",
    "1. **Baseline** — naive full KV cache (the problem)\n",
    "2. **Sliding-Window Attention** — cap cache at a fixed window\n",
    "3. **KV Cache Quantization** — store K/V in INT8 instead of FP16\n",
    "4. **Multi-Query / Grouped-Query Attention (MQA/GQA)** — fewer K/V heads\n",
    "5. **PagedAttention** — allocate cache in non-contiguous pages (vLLM idea)\n",
    "6. **StreamingLLM / Sink Tokens** — keep initial + recent tokens only\n",
    "\n",
    "All demonstrations run on **CPU with small toy models** so every developer can reproduce this without a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 — Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (only needs to run once)\n",
    "import subprocess, sys\n",
    "pkgs = [\"torch\", \"matplotlib\", \"numpy\", \"psutil\", \"tabulate\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + pkgs)\n",
    "print(\"✅ Dependencies ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, psutil, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from tabulate import tabulate\n",
    "\n",
    "torch.manual_seed(42)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "\n",
    "# ── colour palette ──────────────────────────────────────────────────────────\n",
    "C = dict(baseline=\"#e74c3c\", sliding=\"#3498db\", quant=\"#2ecc71\",\n",
    "         gqa=\"#9b59b6\", paged=\"#f39c12\", streaming=\"#1abc9c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 — Understanding KV Cache Memory: The Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_bytes(n_layers: int, n_heads: int, head_dim: int,\n",
    "                   seq_len: int, dtype_bytes: int = 2,\n",
    "                   n_kv_heads: int | None = None) -> int:\n",
    "    \"\"\"\n",
    "    KV cache size in bytes.\n",
    "    2  → K and V\n",
    "    n_kv_heads → for GQA/MQA (defaults to n_heads for standard MHA)\n",
    "    \"\"\"\n",
    "    kv_heads = n_kv_heads if n_kv_heads is not None else n_heads\n",
    "    return 2 * n_layers * kv_heads * head_dim * seq_len * dtype_bytes\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Llama-3 7B\":  dict(n_layers=32, n_heads=32, head_dim=128),\n",
    "    \"Llama-3 13B\": dict(n_layers=40, n_heads=40, head_dim=128),\n",
    "    \"Llama-3 70B\": dict(n_layers=80, n_heads=64, head_dim=128),\n",
    "}\n",
    "seq_lens = [512, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "rows = []\n",
    "for name, cfg in models.items():\n",
    "    row = [name]\n",
    "    for sl in seq_lens:\n",
    "        gb = kv_cache_bytes(**cfg, seq_len=sl) / 1e9\n",
    "        row.append(f\"{gb:.1f} GB\")\n",
    "    rows.append(row)\n",
    "\n",
    "print(tabulate(rows,\n",
    "               headers=[\"Model\"] + [f\"{sl:,} tok\" for sl in seq_lens],\n",
    "               tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 — Toy Transformer Block (shared baseline for all experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard MHA with an optional KV cache returned per step.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int | None = None):\n",
    "        super().__init__()\n",
    "        self.n_heads    = n_heads\n",
    "        self.n_kv_heads = n_kv_heads or n_heads          # MHA: same as n_heads\n",
    "        self.head_dim   = d_model // n_heads\n",
    "        self.scale      = self.head_dim ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(d_model, n_heads    * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(d_model, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, past_k=None, past_v=None):\n",
    "        B, T, C = x.shape\n",
    "        q = self.wq(x).view(B, T, self.n_heads,    self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # ── Append to KV cache ──────────────────────────────────────────────\n",
    "        k = torch.cat([past_k, k], dim=2) if past_k is not None else k\n",
    "        v = torch.cat([past_v, v], dim=2) if past_v is not None else v\n",
    "\n",
    "        # ── GQA: expand KV heads to match Q heads ──────────────────────────\n",
    "        if self.n_kv_heads != self.n_heads:\n",
    "            ratio = self.n_heads // self.n_kv_heads\n",
    "            k = k.repeat_interleave(ratio, dim=1)\n",
    "            v = v.repeat_interleave(ratio, dim=1)\n",
    "\n",
    "        att = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = torch.matmul(att, v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.wo(out), k, v\n",
    "\n",
    "\n",
    "class ToyTransformer(nn.Module):\n",
    "    \"\"\"Single-layer transformer for experimentation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=512, d_model=128, n_heads=4, n_kv_heads=None):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.attn  = MultiHeadAttention(d_model, n_heads, n_kv_heads)\n",
    "        self.ff    = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def step(self, token_id, past_k=None, past_v=None):\n",
    "        \"\"\"Generate one token, returning logits and updated KV cache.\"\"\"\n",
    "        x = self.embed(token_id.unsqueeze(0).unsqueeze(0))   # (1,1,d)\n",
    "        h, k, v = self.attn(self.ln1(x), past_k, past_v)\n",
    "        x = x + h\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        logits = self.head(x[:, -1, :])\n",
    "        return logits, k, v\n",
    "\n",
    "print(\"✅ ToyTransformer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 — Solution 1: Baseline (Full KV Cache) — Demonstrating the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(model, n_tokens: int, strategy: str = \"full\",\n",
    "                   window: int = 64, sink_tokens: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Run autoregressive generation and collect per-step timing and memory.\n",
    "\n",
    "    strategy:\n",
    "        'full'      – standard KV cache (baseline)\n",
    "        'sliding'   – keep only last `window` tokens\n",
    "        'streaming' – keep first `sink_tokens` + last `window` tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latencies, cache_bytes = [], []\n",
    "    token_id = torch.tensor(0, device=DEVICE)\n",
    "    past_k = past_v = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_tokens):\n",
    "            t0 = time.perf_counter()\n",
    "            logits, past_k, past_v = model.step(token_id, past_k, past_v)\n",
    "            latencies.append(time.perf_counter() - t0)\n",
    "\n",
    "            # ── KV cache memory (bytes) ─────────────────────────────────────\n",
    "            cache_bytes.append(past_k.nelement() * past_k.element_size() * 2)  # K+V\n",
    "\n",
    "            # ── Cache pruning ───────────────────────────────────────────────\n",
    "            if strategy == \"sliding\" and past_k.shape[2] > window:\n",
    "                past_k = past_k[:, :, -window:, :]\n",
    "                past_v = past_v[:, :, -window:, :]\n",
    "\n",
    "            elif strategy == \"streaming\":\n",
    "                total = past_k.shape[2]\n",
    "                if total > sink_tokens + window:\n",
    "                    # Sink tokens (first few) + recent window\n",
    "                    past_k = torch.cat([\n",
    "                        past_k[:, :, :sink_tokens, :],\n",
    "                        past_k[:, :, -window:, :]\n",
    "                    ], dim=2)\n",
    "                    past_v = torch.cat([\n",
    "                        past_v[:, :, :sink_tokens, :],\n",
    "                        past_v[:, :, -window:, :]\n",
    "                    ], dim=2)\n",
    "\n",
    "            token_id = logits.argmax(dim=-1).squeeze()\n",
    "\n",
    "    return {\"latencies\": latencies, \"cache_bytes\": cache_bytes}\n",
    "\n",
    "\n",
    "N_TOKENS = 300\n",
    "model_base = ToyTransformer().to(DEVICE)\n",
    "\n",
    "print(\"Running baseline (full KV cache)...\")\n",
    "baseline = run_generation(model_base, N_TOKENS, strategy=\"full\")\n",
    "print(f\"  First token latency : {baseline['latencies'][0]*1000:.2f} ms\")\n",
    "print(f\"  Last  token latency : {baseline['latencies'][-1]*1000:.2f} ms\")\n",
    "print(f\"  Cache at end        : {baseline['cache_bytes'][-1] / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 — Solution 2: Sliding-Window Attention\n",
    "\n",
    "**Idea**: Only attend to the most recent **W** tokens.  \n",
    "Cache size is O(W) instead of O(n).\n",
    "\n",
    "**Trade-off**: Loses very old context.  \n",
    "**Used by**: Mistral-7B (window = 4096), Longformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 50   # keep last 50 tokens\n",
    "\n",
    "print(\"Running sliding-window strategy...\")\n",
    "sliding = run_generation(model_base, N_TOKENS, strategy=\"sliding\", window=WINDOW)\n",
    "print(f\"  Max cache size: {max(sliding['cache_bytes']) / 1024:.1f} KB  (capped at {WINDOW} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 — Solution 3: StreamingLLM / Attention Sinks\n",
    "\n",
    "**Idea**: Keep the first few \"sink\" tokens (they absorb excess softmax probability) **plus** a recent sliding window.  \n",
    "This lets models generalise to infinite context without fine-tuning.\n",
    "\n",
    "**Paper**: *Efficient Streaming Language Models with Attention Sinks* (Xiao et al., 2023)  \n",
    "**Used by**: `vLLM` StreamingLLM mode, TensorRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINK = 4\n",
    "STREAM_WINDOW = 46   # total cache = 4 sinks + 46 recent = 50 tokens\n",
    "\n",
    "print(\"Running StreamingLLM strategy...\")\n",
    "streaming = run_generation(model_base, N_TOKENS, strategy=\"streaming\",\n",
    "                           window=STREAM_WINDOW, sink_tokens=SINK)\n",
    "print(f\"  Max cache size: {max(streaming['cache_bytes']) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 — Solution 4: Grouped-Query Attention (GQA)\n",
    "\n",
    "**Idea**: Instead of one K/V head per Q head (MHA), share K/V heads across groups of Q heads.  \n",
    "- **MQA** (Multi-Query): 1 K/V head for *all* Q heads — maximum saving.\n",
    "- **GQA** (Grouped-Query): G K/V heads for N Q heads — balanced trade-off.\n",
    "\n",
    "**Saving**: Cache shrinks by factor of `n_heads / n_kv_heads`.  \n",
    "**Used by**: Llama-3 (GQA), Mistral (GQA), Falcon (MQA), Gemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gqa_cache_reduction(n_heads: int, n_kv_heads: int) -> float:\n",
    "    return n_kv_heads / n_heads\n",
    "\n",
    "configs = {\n",
    "    \"MHA (baseline)\": (8, 8),\n",
    "    \"GQA-4 groups\":   (8, 4),\n",
    "    \"GQA-2 groups\":   (8, 2),\n",
    "    \"MQA\":            (8, 1),\n",
    "}\n",
    "\n",
    "print(f\"{'Config':<20} {'KV heads':>10} {'Cache vs MHA':>14}\")\n",
    "print(\"-\" * 46)\n",
    "for label, (nh, nkv) in configs.items():\n",
    "    r = gqa_cache_reduction(nh, nkv)\n",
    "    print(f\"{label:<20} {nkv:>10}     {r*100:>8.0f}%\")\n",
    "\n",
    "print()\n",
    "print(\"Running GQA model (8Q heads → 2 KV heads)...\")\n",
    "model_gqa = ToyTransformer(n_heads=4, n_kv_heads=1).to(DEVICE)   # toy: 4Q, 1KV\n",
    "gqa_result = run_generation(model_gqa, N_TOKENS, strategy=\"full\")\n",
    "print(f\"  GQA cache at step {N_TOKENS}: {gqa_result['cache_bytes'][-1] / 1024:.1f} KB\")\n",
    "print(f\"  Baseline cache at step {N_TOKENS}: {baseline['cache_bytes'][-1] / 1024:.1f} KB\")\n",
    "print(f\"  Reduction: {gqa_result['cache_bytes'][-1]/baseline['cache_bytes'][-1]*100:.0f}% of baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 — Solution 5: KV Cache Quantization\n",
    "\n",
    "**Idea**: Store K/V tensors in INT8 (or INT4) instead of FP16/BF16.  \n",
    "**Saving**: 2× (INT8) or 4× (INT4) memory reduction with negligible quality loss.  \n",
    "**Used by**: `bitsandbytes`, `llm.int8()`, `GPTQ`, `AWQ`, `vLLM` KV quant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedKVCache:\n",
    "    \"\"\"\n",
    "    Simulates INT8 quantized KV cache.\n",
    "    Stores K/V as int8 tensors; dequantises on retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.k_int8: torch.Tensor | None = None\n",
    "        self.v_int8: torch.Tensor | None = None\n",
    "        self.k_scale: float = 1.0\n",
    "        self.v_scale: float = 1.0\n",
    "\n",
    "    def _quantize(self, t: torch.Tensor):\n",
    "        scale = t.abs().max().item() / 127.0 + 1e-8\n",
    "        return (t / scale).round().clamp(-128, 127).to(torch.int8), scale\n",
    "\n",
    "    def _dequantize(self, t: torch.Tensor, scale: float) -> torch.Tensor:\n",
    "        return t.float() * scale\n",
    "\n",
    "    def append(self, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"Quantise and append new K/V slice.\"\"\"\n",
    "        k_q, ks = self._quantize(k)\n",
    "        v_q, vs = self._quantize(v)\n",
    "        # Running scale: simplification — use latest slice scale\n",
    "        self.k_scale, self.v_scale = ks, vs\n",
    "        self.k_int8 = k_q if self.k_int8 is None else torch.cat([self.k_int8, k_q], dim=2)\n",
    "        self.v_int8 = v_q if self.v_int8 is None else torch.cat([self.v_int8, v_q], dim=2)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"Return dequantised K/V for attention computation.\"\"\"\n",
    "        return (\n",
    "            self._dequantize(self.k_int8, self.k_scale),\n",
    "            self._dequantize(self.v_int8, self.v_scale),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def bytes_used(self) -> int:\n",
    "        if self.k_int8 is None:\n",
    "            return 0\n",
    "        return (self.k_int8.nelement() + self.v_int8.nelement()) * 1  # int8 = 1 byte\n",
    "\n",
    "\n",
    "def run_quantized_generation(model, n_tokens: int) -> dict:\n",
    "    model.eval()\n",
    "    latencies, cache_bytes = [], []\n",
    "    token_id = torch.tensor(0, device=DEVICE)\n",
    "    qcache = QuantizedKVCache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_tokens):\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            x = model.embed(token_id.unsqueeze(0).unsqueeze(0))\n",
    "            x_norm = model.ln1(x)\n",
    "\n",
    "            # Compute new Q, K, V\n",
    "            attn = model.attn\n",
    "            B, T, _ = x_norm.shape\n",
    "            q = attn.wq(x_norm).view(B, T, attn.n_heads, attn.head_dim).transpose(1, 2)\n",
    "            k = attn.wk(x_norm).view(B, T, attn.n_kv_heads, attn.head_dim).transpose(1, 2)\n",
    "            v = attn.wv(x_norm).view(B, T, attn.n_kv_heads, attn.head_dim).transpose(1, 2)\n",
    "\n",
    "            # Append to quantized cache\n",
    "            qcache.append(k, v)\n",
    "\n",
    "            # Dequantise for attention\n",
    "            k_full, v_full = qcache.get()\n",
    "\n",
    "            att = torch.matmul(q, k_full.transpose(-2, -1)) * attn.scale\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            out = torch.matmul(att, v_full).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "            h   = attn.wo(out)\n",
    "\n",
    "            x = x + h\n",
    "            x = x + model.ff(model.ln2(x))\n",
    "            logits = model.head(x[:, -1, :])\n",
    "\n",
    "            latencies.append(time.perf_counter() - t0)\n",
    "            cache_bytes.append(qcache.bytes_used)\n",
    "            token_id = logits.argmax(dim=-1).squeeze()\n",
    "\n",
    "    return {\"latencies\": latencies, \"cache_bytes\": cache_bytes}\n",
    "\n",
    "\n",
    "print(\"Running INT8 quantized KV cache...\")\n",
    "quant_result = run_quantized_generation(model_base, N_TOKENS)\n",
    "ratio = quant_result['cache_bytes'][-1] / baseline['cache_bytes'][-1]\n",
    "print(f\"  Quantized cache: {quant_result['cache_bytes'][-1] / 1024:.1f} KB\")\n",
    "print(f\"  Baseline cache : {baseline['cache_bytes'][-1] / 1024:.1f} KB\")\n",
    "print(f\"  Memory ratio   : {ratio:.2f}x  (expected ~0.5x for INT8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 — Solution 6: PagedAttention (vLLM concept)\n",
    "\n",
    "**Idea**: Instead of one giant contiguous KV tensor per sequence, divide the cache into fixed-size **pages** (blocks).  \n",
    "A **block table** maps logical token positions to physical memory pages.\n",
    "\n",
    "**Why this matters**:\n",
    "- No memory fragmentation — pages can be scattered anywhere in VRAM.\n",
    "- Multiple requests can **share** pages (prefix caching / prompt caching).\n",
    "- Allows **preemption**: swap pages to CPU, reload when needed.\n",
    "\n",
    "**Used by**: vLLM (original invention), TGI, SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    \"\"\"\n",
    "    Simplified PagedAttention cache.\n",
    "    Memory is pre-allocated as a pool of fixed-size blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_kv_heads: int, head_dim: int,\n",
    "                 block_size: int = 16, max_blocks: int = 200,\n",
    "                 dtype=torch.float32):\n",
    "        self.block_size  = block_size\n",
    "        self.n_kv_heads  = n_kv_heads\n",
    "        self.head_dim    = head_dim\n",
    "\n",
    "        # ── Pre-allocated GPU pool ──────────────────────────────────────────\n",
    "        # Shape: (max_blocks, 2, n_kv_heads, block_size, head_dim)\n",
    "        self.pool = torch.zeros(\n",
    "            max_blocks, 2, n_kv_heads, block_size, head_dim, dtype=dtype\n",
    "        )\n",
    "        self.free_blocks  = list(range(max_blocks))   # block IDs available\n",
    "        self.block_table: list[int] = []              # logical → physical\n",
    "        self.slot_idx     = 0                         # position within current block\n",
    "        self.seq_len      = 0\n",
    "\n",
    "    def _alloc_block(self) -> int:\n",
    "        if not self.free_blocks:\n",
    "            raise RuntimeError(\"Out of KV cache pages!\")\n",
    "        blk = self.free_blocks.pop(0)\n",
    "        self.block_table.append(blk)\n",
    "        return blk\n",
    "\n",
    "    def append(self, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"Write one token's K/V into the next available slot.\"\"\"\n",
    "        if self.slot_idx == 0:\n",
    "            self._alloc_block()\n",
    "\n",
    "        blk = self.block_table[-1]\n",
    "        self.pool[blk, 0, :, self.slot_idx, :] = k.squeeze(0).squeeze(1)\n",
    "        self.pool[blk, 1, :, self.slot_idx, :] = v.squeeze(0).squeeze(1)\n",
    "        self.slot_idx  = (self.slot_idx + 1) % self.block_size\n",
    "        self.seq_len  += 1\n",
    "\n",
    "    def get_contiguous(self):\n",
    "        \"\"\"Gather all K/V into a contiguous (1, H, S, D) tensor for attention.\"\"\"\n",
    "        ks, vs = [], []\n",
    "        for i, blk in enumerate(self.block_table):\n",
    "            end = self.block_size if i < len(self.block_table) - 1 else self.slot_idx or self.block_size\n",
    "            ks.append(self.pool[blk, 0, :, :end, :])   # (H, end, D)\n",
    "            vs.append(self.pool[blk, 1, :, :end, :])\n",
    "        K = torch.cat(ks, dim=1).unsqueeze(0)   # (1, H, S, D)\n",
    "        V = torch.cat(vs, dim=1).unsqueeze(0)\n",
    "        return K, V\n",
    "\n",
    "    @property\n",
    "    def blocks_used(self):\n",
    "        return len(self.block_table)\n",
    "\n",
    "\n",
    "def run_paged_generation(model, n_tokens: int, block_size: int = 16) -> dict:\n",
    "    model.eval()\n",
    "    latencies, cache_bytes, blocks_used = [], [], []\n",
    "    token_id = torch.tensor(0, device=DEVICE)\n",
    "\n",
    "    attn = model.attn\n",
    "    pcache = PagedKVCache(\n",
    "        n_kv_heads=attn.n_kv_heads,\n",
    "        head_dim=attn.head_dim,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_tokens):\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            x = model.embed(token_id.unsqueeze(0).unsqueeze(0))\n",
    "            B, T, _ = x.shape\n",
    "            x_norm = model.ln1(x)\n",
    "\n",
    "            q = attn.wq(x_norm).view(B, T, attn.n_heads, attn.head_dim).transpose(1, 2)\n",
    "            k = attn.wk(x_norm).view(B, T, attn.n_kv_heads, attn.head_dim).transpose(1, 2)\n",
    "            v = attn.wv(x_norm).view(B, T, attn.n_kv_heads, attn.head_dim).transpose(1, 2)\n",
    "\n",
    "            pcache.append(k, v)\n",
    "            k_full, v_full = pcache.get_contiguous()\n",
    "\n",
    "            att = torch.matmul(q, k_full.transpose(-2, -1)) * attn.scale\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            out = torch.matmul(att, v_full).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "            h   = attn.wo(out)\n",
    "\n",
    "            x = x + h\n",
    "            x = x + model.ff(model.ln2(x))\n",
    "            logits = model.head(x[:, -1, :])\n",
    "\n",
    "            latencies.append(time.perf_counter() - t0)\n",
    "            # Memory = only the bytes in actually-used pool slots\n",
    "            used_slots = pcache.seq_len\n",
    "            bytes_used = used_slots * attn.n_kv_heads * attn.head_dim * 2 * 4   # K+V, float32\n",
    "            cache_bytes.append(bytes_used)\n",
    "            blocks_used.append(pcache.blocks_used)\n",
    "            token_id = logits.argmax(dim=-1).squeeze()\n",
    "\n",
    "    return {\"latencies\": latencies, \"cache_bytes\": cache_bytes, \"blocks\": blocks_used}\n",
    "\n",
    "\n",
    "print(\"Running PagedAttention simulation...\")\n",
    "paged_result = run_paged_generation(model_base, N_TOKENS)\n",
    "print(f\"  Blocks used at end: {paged_result['blocks'][-1]}  (block_size=16)\")\n",
    "print(f\"  Effective cache   : {paged_result['cache_bytes'][-1]/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 — Visualisation: All Solutions Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(1, N_TOKENS + 1))\n",
    "\n",
    "def to_kb(lst): return [b / 1024 for b in lst]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"KV Cache Memory Wall — Problem vs Solutions\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "# ── Panel 1: Cache size ──────────────────────────────────────────────────────\n",
    "ax = axes[0]\n",
    "ax.plot(steps, to_kb(baseline[\"cache_bytes\"]),    label=\"Baseline (full cache)\",    color=C[\"baseline\"],  lw=2)\n",
    "ax.plot(steps, to_kb(gqa_result[\"cache_bytes\"]),  label=\"GQA (4Q→1KV head)\",        color=C[\"gqa\"],       lw=2)\n",
    "ax.plot(steps, to_kb(quant_result[\"cache_bytes\"]),label=\"INT8 Quantized\",           color=C[\"quant\"],     lw=2)\n",
    "ax.plot(steps, to_kb(sliding[\"cache_bytes\"]),     label=f\"Sliding Window (W={WINDOW})\", color=C[\"sliding\"], lw=2)\n",
    "ax.plot(steps, to_kb(streaming[\"cache_bytes\"]),   label=f\"StreamingLLM (sink={SINK}, W={STREAM_WINDOW})\",\n",
    "                                                                                    color=C[\"streaming\"], lw=2)\n",
    "ax.plot(steps, to_kb(paged_result[\"cache_bytes\"]),label=\"PagedAttention\",           color=C[\"paged\"],     lw=2, linestyle=\"--\")\n",
    "ax.set_xlabel(\"Generation Step (token #)\")\n",
    "ax.set_ylabel(\"KV Cache Memory (KB)\")\n",
    "ax.set_title(\"Memory Usage vs Generation Step\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ── Panel 2: Latency per step ────────────────────────────────────────────────\n",
    "ax = axes[1]\n",
    "window_smooth = 10\n",
    "def smooth(arr):\n",
    "    return np.convolve(arr, np.ones(window_smooth)/window_smooth, mode=\"same\")\n",
    "\n",
    "def ms(lst): return [x*1000 for x in lst]\n",
    "\n",
    "ax.plot(steps, smooth(ms(baseline[\"latencies\"])),    label=\"Baseline\",       color=C[\"baseline\"],  lw=2)\n",
    "ax.plot(steps, smooth(ms(gqa_result[\"latencies\"])),  label=\"GQA\",            color=C[\"gqa\"],       lw=2)\n",
    "ax.plot(steps, smooth(ms(quant_result[\"latencies\"])),label=\"INT8 Quantized\", color=C[\"quant\"],     lw=2)\n",
    "ax.plot(steps, smooth(ms(sliding[\"latencies\"])),     label=\"Sliding Window\", color=C[\"sliding\"],   lw=2)\n",
    "ax.plot(steps, smooth(ms(streaming[\"latencies\"])),   label=\"StreamingLLM\",   color=C[\"streaming\"], lw=2)\n",
    "ax.set_xlabel(\"Generation Step (token #)\")\n",
    "ax.set_ylabel(\"Step Latency (ms, smoothed)\")\n",
    "ax.set_title(\"Per-Step Latency vs Generation Step\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kv_cache_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Figure saved to kv_cache_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 — Theoretical Memory Savings at Production Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-3 70B at 32k context length\n",
    "cfg = dict(n_layers=80, n_heads=64, head_dim=128)\n",
    "seq = 32768\n",
    "\n",
    "scenarios = {\n",
    "    \"MHA FP16 (baseline)\":          dict(**cfg, dtype_bytes=2),\n",
    "    \"GQA-8 (Llama-3 actual)\": dict(**cfg, n_kv_heads=8,  dtype_bytes=2),\n",
    "    \"MHA INT8 quantized\":            dict(**cfg, dtype_bytes=1),\n",
    "    \"GQA-8 + INT8\":          dict(**cfg, n_kv_heads=8,  dtype_bytes=1),\n",
    "    \"MQA FP16\":                      dict(**cfg, n_kv_heads=1,  dtype_bytes=2),\n",
    "    \"Sliding Window (4k)\": None,  # handled separately\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Scenario':<30} {'Cache (GB)':>12} {'vs Baseline':>14}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_gb = kv_cache_bytes(**cfg, seq_len=seq, dtype_bytes=2) / 1e9\n",
    "\n",
    "for label, kw in scenarios.items():\n",
    "    if kw is None:   # sliding window\n",
    "        gb = kv_cache_bytes(**cfg, seq_len=4096, dtype_bytes=2) / 1e9\n",
    "    else:\n",
    "        gb = kv_cache_bytes(**kw, seq_len=seq) / 1e9\n",
    "    pct = gb / baseline_gb * 100\n",
    "    print(f\"{label:<30} {gb:>10.1f} GB   {pct:>8.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 — Decision Guide: Which Solution When?\n",
    "\n",
    "| Technique | Memory Saving | Quality Impact | Complexity | Best For |\n",
    "|-----------|:---:|:---:|:---:|---|\n",
    "| **GQA/MQA** | 8–64× | Minimal (retrained) | Low | New model training |\n",
    "| **KV INT8 Quant** | ~2× | Negligible | Medium | Existing deployed models |\n",
    "| **KV INT4 Quant** | ~4× | Small | Medium | Memory-constrained inference |\n",
    "| **Sliding Window** | O(W/n) | Loses old context | Low | Chat / streaming |\n",
    "| **StreamingLLM** | O(W/n) | Near-lossless for most tasks | Low | Long streaming generation |\n",
    "| **PagedAttention** | ~0% saving but eliminates fragmentation | None | High | Multi-user serving (vLLM) |\n",
    "| **Offload to CPU/NVMe** | Removes VRAM limit | Latency penalty | High | Very long context |\n",
    "| **Sparse Attention** | Varies | Varies | Very High | Research / custom models |\n",
    "\n",
    "### Production Recommendation Stack\n",
    "\n",
    "```\n",
    "1. Choose a GQA-trained model (e.g., Llama-3, Mistral) → free 8× saving\n",
    "2. Serve with vLLM (PagedAttention) → eliminates VRAM fragmentation\n",
    "3. Enable INT8 KV cache → another 2× saving\n",
    "4. Add sliding window or StreamingLLM for very long sessions\n",
    "```\n",
    "\n",
    "Combined, these can reduce KV cache memory from **64 GB → ~4 GB** for a 70B model at 32k context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary bar chart ────────────────────────────────────────────────────────\n",
    "labels = [\n",
    "    \"Baseline\\nMHA FP16\",\n",
    "    \"GQA-8\\nFP16\",\n",
    "    \"MHA\\nINT8\",\n",
    "    \"GQA-8\\nINT8\",\n",
    "    \"Sliding\\nW=4k\",\n",
    "    \"MQA\\nFP16\",\n",
    "]\n",
    "gbs = [\n",
    "    kv_cache_bytes(**cfg, seq_len=32768, dtype_bytes=2) / 1e9,\n",
    "    kv_cache_bytes(**cfg, n_kv_heads=8, seq_len=32768, dtype_bytes=2) / 1e9,\n",
    "    kv_cache_bytes(**cfg, seq_len=32768, dtype_bytes=1) / 1e9,\n",
    "    kv_cache_bytes(**cfg, n_kv_heads=8, seq_len=32768, dtype_bytes=1) / 1e9,\n",
    "    kv_cache_bytes(**cfg, seq_len=4096,  dtype_bytes=2) / 1e9,\n",
    "    kv_cache_bytes(**cfg, n_kv_heads=1,  seq_len=32768, dtype_bytes=2) / 1e9,\n",
    "]\n",
    "colours = [C[\"baseline\"], C[\"gqa\"], C[\"quant\"], C[\"streaming\"], C[\"sliding\"], C[\"paged\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(labels, gbs, color=colours, edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "for bar, gb in zip(bars, gbs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f\"{gb:.1f} GB\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "ax.set_ylabel(\"KV Cache Memory (GB)\")\n",
    "ax.set_title(\"Llama-3 70B · 32k context · KV Cache Footprint by Strategy\", fontsize=13)\n",
    "ax.set_ylim(0, max(gbs) * 1.18)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kv_cache_strategies_bar.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Figure saved to kv_cache_strategies_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| # | Technique | Core Idea | Where to Apply |\n",
    "|---|-----------|-----------|----------------|\n",
    "| 1 | **Full KV Cache** | Store all past K/V | ❌ The problem — O(n) VRAM |\n",
    "| 2 | **Sliding Window** | Forget tokens older than W | Chat agents, real-time streaming |\n",
    "| 3 | **StreamingLLM / Attention Sinks** | Keep sink + recent | Infinite generation without fine-tuning |\n",
    "| 4 | **GQA / MQA** | Fewer KV heads, shared across Q | Model training time (most impactful) |\n",
    "| 5 | **KV Quantization** | INT8/INT4 for K/V tensors | Drop-in at inference, ~2–4× saving |\n",
    "| 6 | **PagedAttention** | Non-contiguous memory pages | Multi-user serving (vLLM, TGI) |\n",
    "\n",
    "The **state-of-the-art production stack** (e.g. vLLM + Llama-3) combines all of 4 + 5 + 6 to achieve near-linear memory scaling with minimal quality degradation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
